
[{"content":"","date":"26 October 2025","externalUrl":null,"permalink":"/posts/","section":"","summary":"","title":"","type":"posts"},{"content":" As someone without much backrounds in neither physics nor computer science, I find lots of available introductions on transformers very confusing, despite current Aİ bloom (the name \u0026ldquo;GPT\u0026rdquo; is short for \u0026lsquo;Generative Pre-training Transformer\u0026rsquo;!). Most of the articles on transformers focuses on attention mechanisms, using either the OG transformer or the classic BERT as examples. They would spend lot of time talking about embeddings \u0026amp; attetions on the encoding side, and skipped most the decoding by saying \u0026lsquo;well you just do the same thing again and there you have it!\u0026rsquo;. Well that\u0026rsquo;s not very helpful isn\u0026rsquo;t it. Don\u0026rsquo;t get me wrong, there are a lot of very good learning materials out there, for example the amazing interactive transofmer explainer. However, I always find these heavy tutorials not very suitable for my very short attention span or the autism tendency of getting lost in details. The current naming conventions aren\u0026rsquo;t helpful ether: the \u0026lsquo;attention heads\u0026rsquo;, the \u0026lsquo;key/query/value\u0026rsquo;, and all the \u0026lsquo;GEGLU\u0026rsquo;s. They feel like fancy terms to trick shareholders rather than actually describing the model itself. Makes you miss the good old days of \u0026lsquo;RNNs\u0026rsquo; and convolutional networks.\nFinally, I\u0026rsquo;ve decided to bite the bullet and spend some time have a read through the HuggingFace\u0026rsquo;s source code for google\u0026rsquo;s T5 Model, THE encoder-decoder everyone on the steet are talking about. It was a relatively long process with lots of back and forth jumping between classes and methods. I could not emphasis how much I appretiate HuggingFace\u0026rsquo;s maximalist coding choice, where the entire model architecture is contained inside one single .py file. The bonouns point is, I didn\u0026rsquo;t experience the pain of come accross import tensorflow followed by one single if-else check inside dataset iterator\u0026rsquo;. I took lots of notes here and there during the process of studying transformers, think now it\u0026rsquo;s a very good time to share some of my findings. In this (or probably a series of?) blogpost(s?), I am going to collate my past notes on text-specific transformer models piece by piece in a reader-friendly manner. I hope these notes can help others alongside their studying, or being an interesting nice little piece of articles to read through.\nIn this particular post, I would like to do a very brief overview of the transformer model architecture, specifically on the attention mechanism. I won\u0026rsquo;t go metion too much math and there won\u0026rsquo;t be any mathenathical formulas. However, I would assume readers of this silly little post already have some okay-ish background of math/ datascience, and know some very basics of neural network as well as text processing (i.e., matrix computations embeddings, tokens, model fitting etc.). Neural Network Models # In order to make things easier to understand, I would wish to start with an inaccuate premise: we can view neural network models as functions that takes some sort of matrix as inputs, do some sort of matrix computations, and output another matrix as the final result. What makes one neural network different from others is how the computation is carried out. It\u0026rsquo;s like \\(y=a \\times x^2\\) is a different function from \\(y=a \\times sin(x)\\), only that in the case of neural network, both x and y are matrics, and the math is much complicated. When it comes to model training, we are essentially trying to find values gives best fit to the data. There\u0026rsquo;s an important assumption here: just because model fits the data well does not mean the model describes mechanisms behind the data. For example, we definitely can fit \\y=a \\times sin(x) + b) to a normal distribution data (like distribution of customer spendings in McDonald\u0026rsquo;s), and it\u0026rsquo;s prob going to be a pretty good fit, but this does not mean the sine function has anything to do with explaining the normal distribution. A good model does not always need to be description, a good model just needs to be useful for its purpose.\nTransformers are preciesly these kinds of models: they are, surprisingly good at fitting into all sorts of data whilst the math behind the model probably doesn\u0026rsquo;t have much to do with the mechanisms behind. We don\u0026rsquo;t know how transformers works so well for text-based tasks. At least not yet. Originally, transformer was designed as an add-on to the text-processing neural network models in order to tackle with some tricky problems (these problems are not the main forcus of the current blogpost so I\u0026rsquo;m skipping them, but here\u0026rsquo;s a good article if you were interested). We just happened to discover that transformers alone is good enough to solve these problems, we just need to make the transformers much bigger. So that\u0026rsquo;s where the Aİ bloom started: GPT2 solved issues in GPT1 by simply being 10 times bigger; the most-recently open-sourced pretrained GPT-Oss, is 200 times bigger than the previous openpsourced model, GPT2 (note: GPT-OSS is structurlly different from the original GPT2 but the fundamental ideas are the same.). There are even speculations suggesting transformer neural network models can be seen as some sort of universal function approximator. That is, it\u0026rsquo;s capacable of \u0026lsquo;approximate\u0026rsquo; other formulas/ functions with certain degree of accuracy, providing the model itself is big enough (\u0026lsquo;universal approximation theorem\u0026rsquo;). One thing that\u0026rsquo;s very suprising is that, transformers are able to produce pretty impressive results, even if the model wasn\u0026rsquo;t trained for the specific tasks. You can try this out yourself. I\u0026rsquo;ll use Flan-T5 as a demo here. Flan-T5 is a variation of T5 model that fine-tuned on instruction-specific tasks. That is, we can insert some instructions before our prompt and the model shall return different results based on different instructions. Here\u0026rsquo;s path to the model:\ngoogle/flan-t5-base model 1013 1.181778e\u0026#43;06 (this is python by the way)\nBefore starting, you might want to install transformers by running pip install transformers in case you haven\u0026rsquo;t done so. It\u0026rsquo;s a library collecting huge tone of open-sourced transformer models that allows you explore around. very neat.\nAnd here\u0026rsquo;s how you can try it out:\n1from transformers import pipeline 2import pprint # to print indented dictionary 3pipe = pipeline(\u0026#39;text2text-generation\u0026#39;, model=\u0026#34;google/flan-t5-base\u0026#34;) To view the list of tasks the original T5 model fine-tuned on:\n1pprint.pp(pipe.model.config.task_specific_params) Output:\n1{\u0026#39;summarization\u0026#39;: {\u0026#39;early_stopping\u0026#39;: True, 2 \u0026#39;length_penalty\u0026#39;: 2.0, 3 \u0026#39;max_length\u0026#39;: 200, 4 \u0026#39;min_length\u0026#39;: 30, 5 \u0026#39;no_repeat_ngram_size\u0026#39;: 3, 6 \u0026#39;num_beams\u0026#39;: 4, 7 \u0026#39;prefix\u0026#39;: \u0026#39;summarize: \u0026#39;}, 8 \u0026#39;translation_en_to_de\u0026#39;: {\u0026#39;early_stopping\u0026#39;: True, 9 \u0026#39;max_length\u0026#39;: 300, 10 \u0026#39;num_beams\u0026#39;: 4, 11 \u0026#39;prefix\u0026#39;: \u0026#39;translate English to German: \u0026#39;}, 12 \u0026#39;translation_en_to_fr\u0026#39;: {\u0026#39;early_stopping\u0026#39;: True, 13 \u0026#39;max_length\u0026#39;: 300, 14 \u0026#39;num_beams\u0026#39;: 4, 15 \u0026#39;prefix\u0026#39;: \u0026#39;translate English to French: \u0026#39;}, 16 \u0026#39;translation_en_to_ro\u0026#39;: {\u0026#39;early_stopping\u0026#39;: True, 17 \u0026#39;max_length\u0026#39;: 300, 18 \u0026#39;num_beams\u0026#39;: 4, 19 \u0026#39;prefix\u0026#39;: \u0026#39;translate English to Romanian: \u0026#39;}} The \u0026lsquo;prefix\u0026rsquo; here refers to the instruction that the model has been trained on (An example input would be like \u0026lsquo;translate English to German: I LOVE FISH!!!!\u0026rsquo;). And as you can see, the model was trained on translation tasks for English-German, English-French, and English-Romanian. ..So let\u0026rsquo;s try Spanish translation:\n1print(pipe(\u0026#39;translate English to Spanish: I love fish!!!!!\u0026#39;)) Althogh the model was not trained on Spanish translation tasks, it still produced pretty impressive results:\n1[{\u0026#39;generated_text\u0026#39;: \u0026#39;Me encanta el pescado!\u0026#39;}] There are lot of speculations on why model is able to perform such tasks. For exmaple, some researchers do suggest models that are big enough might capture meanings behind words as well as language-specific syntax features, and thus are able to convert one language to another. You can view how big the model in the demo is:\npprint.pp(f\u0026#34;Number of parameters: {pipe.model.num_parameters():,}\u0026#34;) Output:\n\u0026#39;Number of parameters: 247,577,856\u0026#39; Architecture # The general structure of a transformer model lookes like this:\nThe input gets converted into vectors or matrics. This conversion process can vary based on different types of inputs. It can simply be some sort of look-up tables (text embedding), some matrix transformations of the raw inputs (convolution) etc. The raw output from step (1) feeds into the multiple different attention layers. Mathematically, each attention layer is doing very much the same mathematical operation, with each layer having its own sets of parameters. Each layer takes a matrix as an input, and outputs another matrix to pass onto the next layer. This process is repeated multiple times. We convert the matrix output from step (2) into task-specific results we would like. This is usually done by another set of simple matrix operations, depending on the task. For example, if we are doing text sentimental analysis task, this operation could be a simple matrix multiplication, resulting in a final score of 0-10. A typical transformer neural network lookes like this:\n╭─────────────────────────────────────────╮ │ Embedding │ ╰─────────────────────────────────────────╯ ╭─────────────────────────────────────────╮ │ ↓ │ ╰─────────────────────────────────────────╯ ╭──────────── Attention Layer ────────────╮ │ ╭───────────╮╭───────────╮╭───────────╮ │ │ │ Attention ││ Attention ││ │ │ │ │ Head ││ Head ││ ... │ │ │ ╰───────────╯╰───────────╯╰───────────╯ │ ╰─────────────────────────────────────────╯ ╭─────────────────────────────────────────╮ │ ↓ │ ╰─────────────────────────────────────────╯ ╭──────────── Attention Layer ────────────╮ │ ╭───────────╮╭───────────╮╭───────────╮ │ │ │ Attention ││ Attention ││ │ │ │ │ Head ││ Head ││ ... │ │ │ ╰───────────╯╰───────────╯╰───────────╯ │ ╰─────────────────────────────────────────╯ ╭─────────────────────────────────────────╮ │ ↓ │ ╰─────────────────────────────────────────╯ ╭─────────────────────────────────────────╮ │ ... │ ╰─────────────────────────────────────────╯ ╭─────────────────────────────────────────╮ │ ↓ │ ╰─────────────────────────────────────────╯ ╭─────────────────────────────────────────╮ │ Output │ ╰─────────────────────────────────────────╯ What really makes transformers unique (and what\u0026rsquo;s really confusing) is what\u0026rsquo;s happenning in step 2, the so called \u0026lsquo;attention layers\u0026rsquo;. Each attention layer contains multiple attention heads, each of them operated independent from one another: they take output matrix from the previous layer as the input, do some mathematical calculations, and produce some transformed matrics. Matrices output from each of these attention heads are then joined together using some matrix merging functions. The joined matrix is the final output of the current layer.\nAttention head # You can think each of the attention head as a mini neural network: it takes some inputs and spits out some outputs. A typical attention head works like this:\nThe input matrix gets converted into multiple matices through matrix multiplication. Most current transformers converts input matrix into three smaller matrices. Two of the matrix from step (1) gets combined together using some matrix operation. The third matrix from step (1) combines with output from step (2), using some other matrix operation. ╭──────────────────────────────────────────────╮ │ * previous layer * │ ╰──────────────────────────────────────────────╯ ↓ ↓ ╭──────────╮╭──────────╮ │ Matrix 1 ││ Matrix 2 │ ↓ ╰──────────╯╰──────────╯ ↓ ↓ ╭──────────────────────╮╭──────────────────────╮ │ Matrix 1 \u0026amp; 2 Merged ││ Matrix 3 │ ╰──────────────────────╯╰──────────────────────╯ ↓ ↓ ╭──────────────────────────────────────────────╮ │ Matrix 1 \u0026amp; 2 \u0026amp; 3 Re-Joined │ ╰──────────────────────────────────────────────╯ ╭──────────────────────────────────────────────╮ │ ↓ ↓ ↓ │ ╰──────────────────────────────────────────────╯ ╭──────────────────────────────────────────────╮ │ * to be combined * │ ╰──────────────────────────────────────────────╯\nSo where are the parameters? # So what are the parameters to be learned in this monstruous model? Whenever the model does matrix calculations, the model is really just applying different functions to the input. These functions contain their own sets of operations and rules about how the inpiut should be transformed. These are the paramers to be learned during the training. Yes, almost everything can be set as learned parameter. That\u0026rsquo;s one of the resons why AI models are so bloody huge.\nWhy? # You might want to ask, how do we come up with this design? Are they specifically designed to answer some sort of linguistic research questions? Why this particular design work so well? Transformer was designed to tackle with some tricky problems faced by previous neural network designs (the history of transformer architecture is not the main focus for now, but here\u0026rsquo;s a good article if you were interested), however, none of these problems are related to linguistics/ grammar. We don\u0026rsquo;t know how it works nor why it works. At least not yet. For text-based models, some researchers do suggest LLM neural network do capture informations about language features, however, these discoveries are reported after the model was published and prob not the main motivation behind this design. It is a bit facinating that this particular design just works.\n'LLM is magic' ","date":"26 October 2025","externalUrl":null,"permalink":"/posts/1761094597752-wtf-does-decoder-do-hows-that-different-from-the-encoder-/","section":"","summary":"","title":"ELI5 Transformers \u0026 LLMs (part 1?): Attention Mechanisms","type":"posts"},{"content":"","date":"26 October 2025","externalUrl":null,"permalink":"/","section":"Hagfish","summary":"","title":"Hagfish","type":"page"},{"content":"","date":"26 October 2025","externalUrl":null,"permalink":"/tags/huggingface/","section":"Tags","summary":"","title":"Huggingface","type":"tags"},{"content":"","date":"26 October 2025","externalUrl":null,"permalink":"/tags/study-notes/","section":"Tags","summary":"","title":"Study Notes","type":"tags"},{"content":"","date":"26 October 2025","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":"","date":"26 October 2025","externalUrl":null,"permalink":"/tags/transformers/","section":"Tags","summary":"","title":"Transformers","type":"tags"},{"content":"","externalUrl":null,"permalink":"/posts/first-post/","section":"","summary":"","title":"","type":"posts"},{"content":"","externalUrl":null,"permalink":"/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"","externalUrl":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series","type":"series"}]