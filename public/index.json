
[{"content":"","date":"22 October 2025","externalUrl":null,"permalink":"/posts/","section":"","summary":"","title":"","type":"posts"},{"content":"","date":"22 October 2025","externalUrl":null,"permalink":"/","section":"Blowfish","summary":"","title":"Blowfish","type":"page"},{"content":"I always find the idea of \u0026lsquo;decoder\u0026rsquo; being very confusing, specially when it gets topped by an encoder. Most of the articles on transformers focuses on attention mechanisms, using either the OG transformer or the classic BERT as examples. They would spend lot of time talking about embeddings \u0026amp; attetions on the encoding side, and skipped most the decoding by saying \u0026lsquo;well you just do the same thing again and there you have it!\u0026rsquo;. Well that\u0026rsquo;s not very helpful isn\u0026rsquo;t it. Don\u0026rsquo;t get me wrong, there are a lot of very good learning materials out there, for example the amazing interactive transofmer explainer. However, I always find these heavy tutorials not very suitable for my very short attention span or the autism tendency of getting lost in details.\nFinally, I\u0026rsquo;ve decided to bite the bullet and spend some time have a read through the source code of THE encoder-decoder everyone on the steet are talking about. I could not emphasis how much I appretiate HuggingFace\u0026rsquo;s maximalist coding choice, where the entire model architecture is contained inside one single .py file. However, turns out it\u0026rsquo;s still a rather painful process nevertheless. At least, I didn\u0026rsquo;t experience the pain of come accross import tensorflow followed by one single if-else check inside dataset iterator function\u0026rsquo;.\nHere in this post, I\u0026rsquo;ll very quickly go through some of the stuffs I\u0026rsquo;ve learned, and try to explain them as clearly as I can.\nA very quick ELI5 on attention mechanism # In a very simplistic term, a trnsformer neural network can be seen as some sort of universal function approximator. That is, it\u0026rsquo;s capacable of \u0026lsquo;approximate\u0026rsquo; other formulas/ functions with certain degree of accuracy, providing the neural network is big enough (\u0026lsquo;universal approximation theorem\u0026rsquo;). The basic idea is that, a good model does not always need to be descriptive about underlying mechanisms, so long we are only interested in the inputs/ outputs.\n","date":"22 October 2025","externalUrl":null,"permalink":"/posts/1761094597752-wtf-does-decoder-do-hows-that-different-from-the-encoder-/","section":"","summary":"","title":"ELI5: Transformers and Decoders","type":"posts"},{"content":"","date":"22 October 2025","externalUrl":null,"permalink":"/tags/huggingface/","section":"Tags","summary":"","title":"Huggingface","type":"tags"},{"content":"","date":"22 October 2025","externalUrl":null,"permalink":"/tags/study-notes/","section":"Tags","summary":"","title":"Study Notes","type":"tags"},{"content":"","date":"22 October 2025","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":"","date":"22 October 2025","externalUrl":null,"permalink":"/tags/transformers/","section":"Tags","summary":"","title":"Transformers","type":"tags"},{"content":"","externalUrl":null,"permalink":"/posts/first-post/","section":"","summary":"","title":"","type":"posts"},{"content":"","externalUrl":null,"permalink":"/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"","externalUrl":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series","type":"series"}]