
[{"content":"","date":"11 November 2025","externalUrl":null,"permalink":"/posts/","section":"","summary":"","title":"","type":"posts"},{"content":"","date":"11 November 2025","externalUrl":null,"permalink":"/tags/ai/","section":"Tags","summary":"","title":"AI","type":"tags"},{"content":"","date":"11 November 2025","externalUrl":null,"permalink":"/tags/blogging/","section":"Tags","summary":"","title":"Blogging","type":"tags"},{"content":"","date":"11 November 2025","externalUrl":null,"permalink":"/category/","section":"Category","summary":"","title":"Category","type":"category"},{"content":"an example to get you started\nThis is a heading # This is a subheading # This is a subsubheading # This is a subsubsubheading # This is a paragraph with bold and italic text. Check more at Blowfish documentation undefined\n","date":"11 November 2025","externalUrl":null,"permalink":"/posts/create-your-personal-website-part-1---overview/","section":"","summary":"","title":"Creating your personal website (part 1) - Overview","type":"posts"},{"content":"","date":"11 November 2025","externalUrl":null,"permalink":"/tags/eli5/","section":"Tags","summary":"","title":"ELI5","type":"tags"},{"content":" always has been I\u0026rsquo;m pretty sure you\u0026rsquo;ve already heard about someting like this before: \u0026lsquo;generative LLM is just slightly advanced auto complete\u0026rsquo;. Or, something like \u0026lsquo;all it does is predicting what\u0026rsquo;s the most likely next word with all the previous words given\u0026rsquo;.\nToday I would like to invite you think of text generation in a very different perspective, at least it\u0026rsquo;s the persceptive I found helped me the most: text generation is glorified sequence classification. I\u0026rsquo;m going to use GPT-2 model as an example, walk you through the mechanism of text generation with the source code, and show you how text generation actually works.\nPreparation # Download the model # Open termianl and type-in these codes to install depencies in case you haven\u0026rsquo;t done so:\npip install transformers Then in terminal, type \u0026lsquo;python\u0026rsquo; to start python interactive REPL:\npython You should see something like this in your terminal:\nPython 3.13.7 | packaged by conda-forge | (main, Sep 3 2025, 14:24:46) [Clang 19.1.7 ] on darwin Type \u0026#34;help\u0026#34;, \u0026#34;copyright\u0026#34;, \u0026#34;credits\u0026#34; or \u0026#34;license\u0026#34; for more information. \u0026gt;\u0026gt;\u0026gt; Import dependencies:\nimport torch from transformers import pipeline As of November 2025, Pytorch/ Apple still haven\u0026rsquo;t fixed the memory leak issue on Apple Silicon devices (i.e., post-2020 Macbooks). As a result, running models with pytorch for some period of time will gets slower and slower over time. Just for demonstration purpose, I\u0026rsquo;d recomment manually set pytorch device as \u0026lsquo;cpu\u0026rsquo; because of this. Skip this step if you were confident this won\u0026rsquo;t happen. Since we are just doing demonstrations, we can simly set torch device as \u0026lsquo;cpu\u0026rsquo;:\ntorch.set_default_device(\u0026#39;cpu\u0026#39;) # or \u0026#39;cuda\u0026#39; if you\u0026#39;d like to use GPU, would not recommend \u0026#39;mps\u0026#39; (at least for torch\u0026lt;=2.9.1) Download the model:\ncheckpoint = \u0026#34;openai-community/gpt2\u0026#34; device = torch.get_default_device() pipe = pipeline( \u0026#39;text-generation\u0026#39;, model=checkpoint, device=device, max_new_tokens=256, repetition_penalty=3.5, no_repeat_ngram_size=5, do_sample=True, temperature=0.8 ) The openai GPT2 model released on Huggingface doesn\u0026rsquo;t come with some pretty import settings. We\u0026rsquo;ll need to manually amend them first:\npipe.generation_config.pad_token_id = pipe.tokenizer.eos_token_id pipe.generation_config.bos_token_id = pipe.tokenizer.eos_token_id pipe.generation_config.decoder_start_token_id = pipe.tokenizer.eos_token_id You are now all set. Optional: Testing text generation # We\u0026rsquo;ll first test the text-generation model. Here\u0026rsquo;s a little function to help with text generation. Basically, instead of returning the raw output (list of dictionaries), this function extracts the generated text and prints it directly, just for easier reading.\ndef generate(text:str, **generate_kwargs) -\u0026gt; None: \u0026#34;\u0026#34;\u0026#34; Run text generation pipeline and print out the generated text. \u0026#34;\u0026#34;\u0026#34; result = pipe(text, **generate_kwargs) print(result[0][\u0026#39;generated_text\u0026#39;]) \u0026hellip;and now you can use this function to try out text generation yourself:\ngenerate(\u0026#34;Who\u0026#39;s that Pokemon!?!?\u0026#34;, max_new_tokens=5) Since we\u0026rsquo;ve set do_sample=True in our pipeline, text generation is done through random-sampling. Model would generate slightly different answer everytime we run our generation function.\nLet\u0026rsquo;s run 5 of them:\nfor _ in range(5): generate(\u0026#34;Who\u0026#39;s that Pokemon!?!?\u0026#34;, max_new_tokens=5) Your output would be very different from mine:\nWho\u0026#39;s that Pokemon? Haha… This one Who\u0026#39;s that Pokemon? I can\u0026#39;t really recall Who\u0026#39;s that Pokemon? It\u0026#39;ll have a head Who\u0026#39;s that Pokemon? I\u0026#39;m going to catch Who\u0026#39;s that Pokemon? The first week has You can generate much longer text by setting \u0026lsquo;max_new_tokens\u0026rsquo; to a higher number, note that the model basically generates typical Aİ slops when it\u0026rsquo;s too large:\ngenerate(\u0026#34;Who\u0026#39;s that Pokemon!?!?\u0026#34;, max_new_tokens=1024) Recap on transformer architecture # As mentioned in the previous post,\n1 - Inputs \u0026lt;-\u0026gt; outputs # 2 - \u0026lsquo;Task Head\u0026rsquo; # Model generation vs text classification # (Here is where I write about LM head)\nPopular text generation strategies # 预告？？？parallel training # ","date":"11 November 2025","externalUrl":null,"permalink":"/posts/transformers-pt-2/","section":"","summary":"(it\u0026rsquo;s glorified sequence classification)","title":"ELI5 Transformers (Part 2) - Generation","type":"posts"},{"content":"","date":"11 November 2025","externalUrl":null,"permalink":"/category/eli5-transformers/","section":"Category","summary":"","title":"ELI5-Transformers","type":"category"},{"content":"","date":"11 November 2025","externalUrl":null,"permalink":"/tags/html/","section":"Tags","summary":"","title":"Html","type":"tags"},{"content":"","date":"11 November 2025","externalUrl":null,"permalink":"/tags/huggingface/","section":"Tags","summary":"","title":"Huggingface","type":"tags"},{"content":"","date":"11 November 2025","externalUrl":null,"permalink":"/tags/hugo/","section":"Tags","summary":"","title":"Hugo","type":"tags"},{"content":"","date":"11 November 2025","externalUrl":null,"permalink":"/","section":"Myxiniformes Moment","summary":"","title":"Myxiniformes Moment","type":"page"},{"content":"","date":"11 November 2025","externalUrl":null,"permalink":"/tags/pytorch/","section":"Tags","summary":"","title":"Pytorch","type":"tags"},{"content":"","date":"11 November 2025","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":"","date":"11 November 2025","externalUrl":null,"permalink":"/category/website-building/","section":"Category","summary":"","title":"Website-Building","type":"category"},{"content":" As someone without much backrounds in neither physics nor computer science, I find lots of available introductions on transformers very confusing. Transformers is the talk of the street, GPT is short for \u0026lsquo;Generative Pre-training Transformer\u0026rsquo;! However, most of the articles on transformers focuses on attention mechanisms, using either the OG transformer or the classic BERT as examples. They would spend lot of time talking about embeddings \u0026amp; attetions on the encoding side, and skipped most the decoding by saying \u0026lsquo;well you just do the same thing again and there you have it!\u0026rsquo;. Well that\u0026rsquo;s not very helpful isn\u0026rsquo;t it. Don\u0026rsquo;t get me wrong, there are a lot of very good learning materials out there, for example the amazing interactive transofmer explainer. However, I always find these heavy tutorials not very suitable for my very short attention span or the autism tendency of getting lost in details.\nFinally, I\u0026rsquo;ve decided to bite the bullet and spend some time have a read through the HuggingFace\u0026rsquo;s source code for google\u0026rsquo;s T5 Model, THE encoder-decoder everyone on the steet are talking about. It was a relatively long process with lots of back and forth jumping between classes and methods. I could not emphasis how much I appretiate HuggingFace\u0026rsquo;s maximalist coding choice, where the entire model architecture is contained inside one single .py file. The bonouns point is, I didn\u0026rsquo;t experience the pain of come accross import tensorflow inside dataset iteration. I took lots of notes here and there during the process of studying transformers, think now it\u0026rsquo;s a very good time to share some of my findings. In this (or probably a series of?) blogpost(s?), I am going to collate my past notes on text-specific transformer models piece by piece in a reader-friendly manner. I hope these notes can help others alongside their studying, or being an interesting nice little piece of articles to read through.\nIn this particular post, I would like to do a very brief overview of the transformer model architecture, specifically on the attention mechanism. I won\u0026rsquo;t go metion too much math and there won\u0026rsquo;t be any mathenathical formulas. However, I would assume readers of this silly little post already have some okay-ish background of math/ datascience. (i.e., matrix computations embeddings, tokens, model fitting etc.). I am not going to list out all the implemention details for transformers, since there are a lot of very good materials out there and they are doing fantastic jobs. Instead, in this (maybe series of?) post, İ would like to draw out a general framework on transformers to help one understand the detailed math behind.\nIn this particular blogpost, I\u0026rsquo;ll very quickly go through some very basics on neural network model, just enough to cover what needed for this post, accompied by demo of transformer model as a proof of concept. In this section, there will be some codes that you can copy and paste into an interactive python session to fiddle around for a bit. And lastly, I\u0026rsquo;ll do a quick sketch on the general architecture of transformers, and a overview of the attention mechasm.\nA good model only needs to be useful # In order to make things easier to understand, I would wish to start with an inaccuate premise: we can view neural network models as functions that takes some sort of matrix as inputs, do some sort of matrix computations, and output another matrix as the final result. What makes one neural network different from others is how the computation is carried out. It\u0026rsquo;s like \\(y=a \\times x^2\\) is a different function from \\(y=a \\times sin(x)\\), only that in the case of neural network, both x and y are matrics, and the math is much complicated. When it comes to model training, we are essentially trying to find values gives best fit to the data.\nThere\u0026rsquo;s an important assumption here: just because model fits the data well does not mean the model describes mechanisms behind the data. For example, we definitely can fit \\(y=a \\times sin(x) + b\\) to a normal distribution data (like distribution of customer spendings in McDonald\u0026rsquo;s), and it\u0026rsquo;s prob going to be a pretty good fit, but this does not mean the sine function has anything to do with explaining the normal distribution. A good model does not always need to be description, a good model just needs to be useful for its purpose.\nTransformers are preciesly these kinds of models: they are, surprisingly good at fitting into all sorts of data whilst the math behind the model probably doesn\u0026rsquo;t have much to do with the mechanisms behind. We don\u0026rsquo;t know how transformers works so well for text-based tasks. At least not yet. Originally, transformer was designed as an add-on to the text-processing neural network models in order to tackle with some tricky problems (these problems are not the main forcus of the current blogpost so I\u0026rsquo;m skipping them, but here\u0026rsquo;s a good article if you were interested). We just happened to discover that transformers alone is good enough to solve these problems, we just need to make the transformers much bigger. So that\u0026rsquo;s where the Aİ bloom started: GPT2 solved issues in GPT1 by simply being 10 times bigger; the most-recently open-sourced pretrained GPT-Oss, is 200 times bigger than the previous openpsourced model, GPT2 (note: GPT-OSS is structurlly different from the original GPT2 but the fundamental ideas are the same.). There are even speculations suggesting transformer neural network models can be seen as some sort of universal function approximator. That is, it\u0026rsquo;s capacable of \u0026lsquo;approximate\u0026rsquo; other formulas/ functions with certain degree of accuracy, providing the model itself is big enough (\u0026lsquo;universal approximation theorem\u0026rsquo;). Try it out! Its uncanny (and magical) # It is preciesly the reason why it\u0026rsquo;s very suprising is that, transformers are able to produce pretty impressive results for tasks model that are not specifically trained for. You can try this out yourself. I\u0026rsquo;ll use Flan-T5 as a demo here. Flan-T5 is a variation of T5 model that fine-tuned on instruction-specific tasks. That is, we can insert some instructions before our prompt and the model shall return different results based on different instructions.\nI\u0026rsquo;ll use Python for the demo here because it\u0026rsquo;s convinent. Before starting, you might want to install transformers if you haven\u0026rsquo;t done aleady. It\u0026rsquo;s a library collecting huge tone of open-sourced transformer models that allows you explore around.\nOpen terminal, run this command to install transformers:\npip install transformers The model I am going to use is T5, released by Google a couple of years ago Here\u0026rsquo;s huggingface\u0026rsquo;s link to the model: google/flan-t5-base model 1013 1.181778e\u0026#43;06 In python, run these lines to download \u0026amp; initialise the model:\nfrom transformers import pipeline import pprint # to print indented dictionary pipe = pipeline(\u0026#39;text2text-generation\u0026#39;, model=\u0026#34;google/flan-t5-base\u0026#34;) T5 is one of the very few models that comes with very well-documented records on what kind of task the particular model has been trained on. To view the list of tasks the original T5 model fine-tuned on, we can check the \u0026rsquo;task_specific_params\u0026rsquo; attribute in model.config:\npprint.pp(pipe.model.config.task_specific_params) Output:\n{\u0026#39;summarization\u0026#39;: {\u0026#39;early_stopping\u0026#39;: True, \u0026#39;length_penalty\u0026#39;: 2.0, \u0026#39;max_length\u0026#39;: 200, \u0026#39;min_length\u0026#39;: 30, \u0026#39;no_repeat_ngram_size\u0026#39;: 3, \u0026#39;num_beams\u0026#39;: 4, \u0026#39;prefix\u0026#39;: \u0026#39;summarize: \u0026#39;}, \u0026#39;translation_en_to_de\u0026#39;: {\u0026#39;early_stopping\u0026#39;: True, \u0026#39;max_length\u0026#39;: 300, \u0026#39;num_beams\u0026#39;: 4, \u0026#39;prefix\u0026#39;: \u0026#39;translate English to German: \u0026#39;}, \u0026#39;translation_en_to_fr\u0026#39;: {\u0026#39;early_stopping\u0026#39;: True, \u0026#39;max_length\u0026#39;: 300, \u0026#39;num_beams\u0026#39;: 4, \u0026#39;prefix\u0026#39;: \u0026#39;translate English to French: \u0026#39;}, \u0026#39;translation_en_to_ro\u0026#39;: {\u0026#39;early_stopping\u0026#39;: True, \u0026#39;max_length\u0026#39;: 300, \u0026#39;num_beams\u0026#39;: 4, \u0026#39;prefix\u0026#39;: \u0026#39;translate English to Romanian: \u0026#39;}} These configurations are task-specific parameters for text-generation. Each item in the dictionary (\u0026rsquo;translation_en_to_de\u0026rsquo;, \u0026lsquo;summarization\u0026rsquo; etc., ) corresponds to each text-generation task that the model been previously trained on. As shown in the code block above, the particular model we are testing today was trained on summarization, and three translation tasks: English-German, English-French, and English-Romanian.What we are interested is the \u0026lsquo;prefix\u0026rsquo; key under task name (i.e., \u0026rsquo;translate English to Romanian: \u0026lsquo;). These are the texts that inserted at the beginning of every text input, as extra instructions of telling our model a bit more information about what it should do.\nFor example, if one wants to do english-to-romanian translation, the model input would be converted as follows:\n\u0026lsquo;I LOVE FISH!!!!\u0026rsquo; \u0026ndash;\u0026gt; \u0026lsquo;translate English to German: I LOVE FISH!!!!\u0026rsquo;\n..So instead of languages model already trained on, let\u0026rsquo;s try something model never trined before: English to Spanish translation:\nprint(pipe(\u0026#39;translate English to Spanish: I love fish!!!!!\u0026#39;)) Althogh the model was not trained on Spanish translation tasks, it still produced pretty impressive results:\n[{\u0026#39;generated_text\u0026#39;: \u0026#39;Me encanta el pescado!\u0026#39;}] It\u0026rsquo;s very uncanny that model is able to do things us human did not ask it to do. There are lot of speculations on why model is able to perform such tasks. For exmaple, some researchers do suggest models that are big enough might capture meanings behind words as well as language-specific syntax features, and thus are able to convert one language to another. You can view how big the model in the demo is:\npprint.pp(f\u0026#34;Number of parameters: {pipe.model.num_parameters():,}\u0026#34;) Output:\n\u0026#39;Number of parameters: 247,577,856\u0026#39; \u0026lsquo;LLM is magic\u0026rsquo; The Transformer Architecture # We\u0026rsquo;ll now take a look inside the transformer models and see what kind of math calculations is hapenning. pytorch, the python package that the T5 model in this demo is based off, provies very good tool for visulising model structures Using models mentioned from the previous section, if you want to look at what the model architecture, you can do so by running:\npprint.pp(pipe.model) \u0026hellip;which in term will give you this monstrous output:\nT5ForConditionalGeneration( (shared): Embedding(32128, 768) (encoder): T5Stack( (embed_tokens): Embedding(32128, 768) (block): ModuleList( (0): T5Block( (layer): ModuleList( (0): T5LayerSelfAttention( (SelfAttention): T5Attention( (q): Linear(in_features=768, out_features=768, bias=False) (k): Linear(in_features=768, out_features=768, bias=False) (v): Linear(in_features=768, out_features=768, bias=False) (o): Linear(in_features=768, out_features=768, bias=False) (relative_attention_bias): Embedding(32, 12) ) (layer_norm): T5LayerNorm() (dropout): Dropout(p=0.1, inplace=False) ) (1): T5LayerFF( (DenseReluDense): T5DenseGatedActDense( (wi_0): Linear(in_features=768, out_features=2048, bias=False) (wi_1): Linear(in_features=768, out_features=2048, bias=False) (wo): Linear(in_features=2048, out_features=768, bias=False) (dropout): Dropout(p=0.1, inplace=False) (act): NewGELUActivation() ) (layer_norm): T5LayerNorm() (dropout): Dropout(p=0.1, inplace=False) ) ) ) (1-11): 11 x T5Block( (layer): ModuleList( (0): T5LayerSelfAttention( (SelfAttention): T5Attention( (q): Linear(in_features=768, out_features=768, bias=False) (k): Linear(in_features=768, out_features=768, bias=False) (v): Linear(in_features=768, out_features=768, bias=False) (o): Linear(in_features=768, out_features=768, bias=False) ) (layer_norm): T5LayerNorm() (dropout): Dropout(p=0.1, inplace=False) ) (1): T5LayerFF( (DenseReluDense): T5DenseGatedActDense( (wi_0): Linear(in_features=768, out_features=2048, bias=False) (wi_1): Linear(in_features=768, out_features=2048, bias=False) (wo): Linear(in_features=2048, out_features=768, bias=False) (dropout): Dropout(p=0.1, inplace=False) (act): NewGELUActivation() ) (layer_norm): T5LayerNorm() (dropout): Dropout(p=0.1, inplace=False) ) ) ) ) (final_layer_norm): T5LayerNorm() (dropout): Dropout(p=0.1, inplace=False) ) (decoder): T5Stack( (embed_tokens): Embedding(32128, 768) (block): ModuleList( (0): T5Block( (layer): ModuleList( (0): T5LayerSelfAttention( (SelfAttention): T5Attention( (q): Linear(in_features=768, out_features=768, bias=False) (k): Linear(in_features=768, out_features=768, bias=False) (v): Linear(in_features=768, out_features=768, bias=False) (o): Linear(in_features=768, out_features=768, bias=False) (relative_attention_bias): Embedding(32, 12) ) (layer_norm): T5LayerNorm() (dropout): Dropout(p=0.1, inplace=False) ) (1): T5LayerCrossAttention( (EncDecAttention): T5Attention( (q): Linear(in_features=768, out_features=768, bias=False) (k): Linear(in_features=768, out_features=768, bias=False) (v): Linear(in_features=768, out_features=768, bias=False) (o): Linear(in_features=768, out_features=768, bias=False) ) (layer_norm): T5LayerNorm() (dropout): Dropout(p=0.1, inplace=False) ) (2): T5LayerFF( (DenseReluDense): T5DenseGatedActDense( (wi_0): Linear(in_features=768, out_features=2048, bias=False) (wi_1): Linear(in_features=768, out_features=2048, bias=False) (wo): Linear(in_features=2048, out_features=768, bias=False) (dropout): Dropout(p=0.1, inplace=False) (act): NewGELUActivation() ) (layer_norm): T5LayerNorm() (dropout): Dropout(p=0.1, inplace=False) ) ) ) (1-11): 11 x T5Block( (layer): ModuleList( (0): T5LayerSelfAttention( (SelfAttention): T5Attention( (q): Linear(in_features=768, out_features=768, bias=False) (k): Linear(in_features=768, out_features=768, bias=False) (v): Linear(in_features=768, out_features=768, bias=False) (o): Linear(in_features=768, out_features=768, bias=False) ) (layer_norm): T5LayerNorm() (dropout): Dropout(p=0.1, inplace=False) ) (1): T5LayerCrossAttention( (EncDecAttention): T5Attention( (q): Linear(in_features=768, out_features=768, bias=False) (k): Linear(in_features=768, out_features=768, bias=False) (v): Linear(in_features=768, out_features=768, bias=False) (o): Linear(in_features=768, out_features=768, bias=False) ) (layer_norm): T5LayerNorm() (dropout): Dropout(p=0.1, inplace=False) ) (2): T5LayerFF( (DenseReluDense): T5DenseGatedActDense( (wi_0): Linear(in_features=768, out_features=2048, bias=False) (wi_1): Linear(in_features=768, out_features=2048, bias=False) (wo): Linear(in_features=2048, out_features=768, bias=False) (dropout): Dropout(p=0.1, inplace=False) (act): NewGELUActivation() ) (layer_norm): T5LayerNorm() (dropout): Dropout(p=0.1, inplace=False) ) ) ) ) (final_layer_norm): T5LayerNorm() (dropout): Dropout(p=0.1, inplace=False) ) (lm_head): Linear(in_features=768, out_features=32128, bias=False) ) T5 model belongs to a sub-category of transformers called \u0026rsquo;encoder-decoder\u0026rsquo;. That is, it\u0026rsquo;s a combination of an encoder followed by a decoder. This post is a general introductions of the basics, and I\u0026rsquo;ll talk about encoder/ decoders in more detail in more detail about this in the future.\nLayer-by-Layer # The general structure of a transformer model lookes like this:\nThe input gets converted into vectors or matrics. This conversion process can vary based on different types of inputs. It can simply be some sort of look-up tables (text embedding), some matrix transformations of the raw inputs (convolution) etc. The raw output from step (1) feeds into the multiple different attention layers. Mathematically, each attention layer is doing very much the same mathematical operation, with each layer having its own sets of parameters. Each layer takes a matrix as an input, and outputs another matrix to pass onto the next layer. This process is repeated multiple times. We convert the matrix output from step (2) into task-specific results we would like. This is usually done by another set of simple matrix operations, depending on the task. For example, if we are doing text sentimental analysis task, this operation could be a simple matrix multiplication, resulting in a final score of 0-10. A typical transformer neural network lookes like this:\n╭─────────────────────────────────────────╮ │ Embedding │ ╰─────────────────────────────────────────╯ ╭─────────────────────────────────────────╮ │ ↓ │ ╰─────────────────────────────────────────╯ ╭──────────── Attention Layer ────────────╮ │ ╭───────────╮╭───────────╮╭───────────╮ │ │ │ Attention ││ Attention ││ │ │ │ │ Head ││ Head ││ ... │ │ │ ╰───────────╯╰───────────╯╰───────────╯ │ ╰─────────────────────────────────────────╯ ╭─────────────────────────────────────────╮ │ ↓ │ ╰─────────────────────────────────────────╯ ╭──────────── Attention Layer ────────────╮ │ ╭───────────╮╭───────────╮╭───────────╮ │ │ │ Attention ││ Attention ││ │ │ │ │ Head ││ Head ││ ... │ │ │ ╰───────────╯╰───────────╯╰───────────╯ │ ╰─────────────────────────────────────────╯ ╭─────────────────────────────────────────╮ │ ↓ │ ╰─────────────────────────────────────────╯ ╭─────────────────────────────────────────╮ │ ... │ ╰─────────────────────────────────────────╯ ╭─────────────────────────────────────────╮ │ ↓ │ ╰─────────────────────────────────────────╯ ╭─────────────────────────────────────────╮ │ Output │ ╰─────────────────────────────────────────╯ What really makes transformers unique (and what\u0026rsquo;s really confusing) is what\u0026rsquo;s happenning in step 2, the so called \u0026lsquo;attention layers\u0026rsquo;. Each attention layer contains multiple attention heads, each of them operated independent from one another: they take output matrix from the previous layer as the input, do some mathematical calculations, and produce some transformed matrics. Matrices output from each of these attention heads are then joined together using some matrix merging functions. The joined matrix is the final output of the current layer.\nAttention head # You can think each of the attention head as a mini neural network: it takes some inputs and spits out some outputs. A typical attention head works like this:\nThe input matrix gets converted into multiple matices through matrix multiplication. Most current transformers converts input matrix into three smaller matrices. Two of the matrix from step (1) gets combined together using some matrix operation, usually dot products. The third matrix from step (1) combines with output from step (2), using some other matrix operation. ╭──────────────────────────────────────────────╮ │ * previous layer * │ ╰──────────────────────────────────────────────╯ ↓ ↓ ╭──────────╮╭──────────╮ │ Matrix 1 ││ Matrix 2 │ ↓ ╰──────────╯╰──────────╯ ↓ ↓ ╭──────────────────────╮╭──────────────────────╮ │ Matrix 1 \u0026amp; 2 Merged ││ Matrix 3 │ ╰──────────────────────╯╰──────────────────────╯ ↓ ↓ ╭──────────────────────────────────────────────╮ │ Matrix 1 \u0026amp; 2 \u0026amp; 3 Re-Joined │ ╰──────────────────────────────────────────────╯ ╭──────────────────────────────────────────────╮ │ ↓ ↓ ↓ │ ╰──────────────────────────────────────────────╯ ╭──────────────────────────────────────────────╮ │ * to be combined * │ ╰──────────────────────────────────────────────╯\nWhy are the models so big? # I wanted to point out the (maybe) obvious thing here: almost all operations mentioned contain learnable parameters. Inside individual attention heads, the three matrics convreted by inputs are typically converted by multiplying (\u0026lsquo;dot product\u0026rsquo;) inputs with three separate matrices. These matrics are part of the learnable parameters for the attention head. When we combine matrices, the combination operation also has their own learnable parameters. Furthermore, when we combining outputs from each \u0026lsquo;attention heads\u0026rsquo;, this combining opearation also has its own set of trainable parameters, so on and so on\u0026hellip;Almost every stage of the matrix computations are parameterised, resulting the unbelievably massive Aİ models as of today.\nSo it\u0026rsquo;s just a huge stacks of matirx calculations?\nIn sense yes, it is. The three matrics in the attention head are commonly named as \u0026lsquo;key\u0026rsquo;. \u0026lsquo;query\u0026rsquo; and \u0026lsquo;value\u0026rsquo;, the idea behind these names are: \u0026lsquo;user queries something, program looks for keys (i.e., keywords) to match with query, and value is whatever gets matched with\u0026rsquo;. Honstly I found this explaination very confusing, although by design, it does (sort of) work in such way. My main skeptcisim is that, just because we vaguely designed it this way does not mean it is really what\u0026rsquo;s happening underneath. As shown in the demo earlier, a model that\u0026rsquo;s not trained for English-Spanish translation did have some ability to do English-Spanish translation, innit. We gave names to these matrics, we don\u0026rsquo;t know much about their behaviour.\nWhat\u0026rsquo;s up next?\nI think this is a rather good place to conclude this post, so I\u0026rsquo;ll leave it here for now. I hope you find it helpfull.\nThere are still a bit more stuffs that I\u0026rsquo;d like to share, like how text-generation works and what\u0026rsquo;s really happenning when we are training a generative model. We\u0026rsquo;ve heard of the same old things over and over: \u0026lsquo;generative LLM is just a very massive auto-complete!\u0026rsquo;. Whilst I do aggree with it, I also find it not helpful if one wants to understand text generation, for both model training and model inference. If time alows\u0026hellip;.\nSome Other Resources # I hope whoever come accross this post would find it useful. Here are some extra reading materials that İ found particularly useful:\nPytorch\u0026rsquo;s step-byp-step guide on creating a generative LLM is prob one of the best out there that teaches you all the fundenmentals. BertViz, a very good visualisation tool for looking at attention heads layer by layer. You can run it interactively in a jupyter notebook. Huggingface\u0026rsquo;s LLM cources. Although they tends to focus on the programming \u0026amp; practical applications, I found many of their conceptual guides very good for a beginner. ","date":"31 October 2025","externalUrl":null,"permalink":"/posts/transformers-pt-1/","section":"","summary":"(it\u0026rsquo;s glorified linear algebra)","title":"ELI5 Transformers (Part 1): Attention Mechanism ","type":"posts"},{"content":"","externalUrl":null,"permalink":"/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"","externalUrl":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series","type":"series"}]